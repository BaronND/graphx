{"name":"GraphX","tagline":"Unifying Graphs and Tables","body":"# GraphX: Unifying Graphs and Tables\r\n\r\nGraphX extends the distributed fault-tolerant collections API and\r\ninteractive console of [Spark](http://spark.incubator.apache.org) with\r\na new graph API which leverages recent advances in graph systems\r\n(e.g., [GraphLab](http://graphlab.org)) to enable users to easily and\r\ninteractively build, transform, and reason about graph structured data\r\nat scale.\r\n\r\n\r\n## Motivation\r\n\r\nFrom social networks and targeted advertising to protein modeling and\r\nastrophysics, big graphs capture the structure in data and are central\r\nto the recent advances in machine learning and data mining. Directly\r\napplying existing *data-parallel* tools (e.g.,\r\n[Hadoop](http://hadoop.apache.org) and\r\n[Spark](http://spark.incubator.apache.org)) to graph computation tasks\r\ncan be cumbersome and inefficient.  The need for intuitive, scalable\r\ntools for graph computation has lead to the development of new\r\n*graph-parallel* systems (e.g.,\r\n[Pregel](http://http://giraph.apache.org) and\r\n[GraphLab](http://graphlab.org)) which are designed to efficiently\r\nexecute graph algorithms.  Unfortunately, these systems do not address\r\nthe challenges of graph construction and transformation and provide\r\nlimited fault-tolerance and support for interactive analysis.\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://raw.github.com/jegonzal/graphx/Documentation/docs/img/data_parallel_vs_graph_parallel.png\" />\r\n</p>\r\n\r\n\r\n\r\n## Solution\r\n\r\nThe GraphX project combines the advantages of both data-parallel and\r\ngraph-parallel systems by efficiently expressing graph computation\r\nwithin the [Spark](http://spark.incubator.apache.org) framework.  We\r\nleverage new ideas in distributed graph representation to efficiently\r\ndistribute graphs as tabular data-structures.  Similarly, we leverage\r\nadvances in data-flow systems to exploit in-memory computation and\r\nfault-tolerance.  We provide powerful new operations to simplify graph\r\nconstruction and transformation.  Using these primitives we implement\r\nthe PowerGraph and Pregel abstractions in less than 20 lines of code.\r\nFinally, by exploiting the Scala foundation of Spark, we enable users\r\nto interactively load, transform, and compute on massive graphs.\r\n\r\n<p align=\"center\">\r\n  <img src=\"https://raw.github.com/jegonzal/graphx/Documentation/docs/img/tables_and_graphs.png\" />\r\n</p>\r\n\r\n## Examples\r\n\r\nSuppose I want to build a graph from some text files, restrict the graph \r\nto important relationships and users, run page-rank on the sub-graph, and\r\nthen finally return attributes associated with the top users.  I can do \r\nall of this in just a few lines with GraphX:\r\n\r\n```scala\r\n// Connect to the Spark cluster\r\nval sc = new SparkContext(\"spark://master.amplab.org\", \"research\")\r\n\r\n// Load my user data and prase into tuples of user id and attribute list\r\nval users = sc.textFile(\"hdfs://user_attributes.tsv\")\r\n  .map(line => line.split).map( parts => (parts.head, parts.tail) )\r\n\r\n// Parse the edge data which is already in userId -> userId format\r\nval followerGraph = Graph.textFile(sc, \"hdfs://followers.tsv\")\r\n\r\n// Attach the user attributes\r\nval graph = followerGraph.outerJoinVertices(users){ \r\n  case (uid, deg, Some(attrList)) => attrList\r\n  // Some users may not have attributes so we set them as empty\r\n  case (uid, deg, None) => Array.empty[String] \r\n  }\r\n\r\n// Restrict the graph to users which have exactly two attributes\r\nval subgraph = graph.subgraph((vid, attr) => attr.size == 2)\r\n\r\n// Compute the PageRank \r\nval pagerankGraph = Analytics.pagerank(subgraph)\r\n\r\n// Get the attributes of the top pagerank users\r\nval userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices){\r\n  case (uid, attrList, Some(pr)) => (pr, attrList)\r\n  case (uid, attrList, None) => (pr, attrList)\r\n  }\r\n  \r\nprintln(userInfoWithPageRank.top(5))\r\n\r\n```\r\n\r\n\r\n## Online Documentation\r\n\r\nYou can find the latest Spark documentation, including a programming\r\nguide, on the project webpage at\r\n<http://spark.incubator.apache.org/documentation.html>.  This README\r\nfile only contains basic setup instructions.\r\n\r\n\r\n## Building\r\n\r\nSpark requires Scala 2.9.3 (Scala 2.10 is not yet supported). The\r\nproject is built using Simple Build Tool (SBT), which is packaged with\r\nit. To build Spark and its example programs, run:\r\n\r\n    sbt/sbt assembly\r\n\r\nOnce you've built Spark, the easiest way to start using it is the\r\nshell:\r\n\r\n    ./spark-shell\r\n\r\nOr, for the Python API, the Python shell (`./pyspark`).\r\n\r\nSpark also comes with several sample programs in the `examples`\r\ndirectory.  To run one of them, use `./run-example <class>\r\n<params>`. For example:\r\n\r\n    ./run-example org.apache.spark.examples.SparkLR local[2]\r\n\r\nwill run the Logistic Regression example locally on 2 CPUs.\r\n\r\nEach of the example programs prints usage help if no params are given.\r\n\r\nAll of the Spark samples take a `<master>` parameter that is the\r\ncluster URL to connect to. This can be a mesos:// or spark:// URL, or\r\n\"local\" to run locally with one thread, or \"local[N]\" to run locally\r\nwith N threads.\r\n\r\n\r\n## A Note About Hadoop Versions\r\n\r\nSpark uses the Hadoop core library to talk to HDFS and other\r\nHadoop-supported storage systems. Because the protocols have changed\r\nin different versions of Hadoop, you must build Spark against the same\r\nversion that your cluster runs.  You can change the version by setting\r\nthe `SPARK_HADOOP_VERSION` environment when building Spark.\r\n\r\nFor Apache Hadoop versions 1.x, Cloudera CDH MRv1, and other Hadoop\r\nversions without YARN, use:\r\n\r\n    # Apache Hadoop 1.2.1\r\n    $ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly\r\n\r\n    # Cloudera CDH 4.2.0 with MapReduce v1\r\n    $ SPARK_HADOOP_VERSION=2.0.0-mr1-cdh4.2.0 sbt/sbt assembly\r\n\r\nFor Apache Hadoop 2.x, 0.23.x, Cloudera CDH MRv2, and other Hadoop versions\r\nwith YARN, also set `SPARK_YARN=true`:\r\n\r\n    # Apache Hadoop 2.0.5-alpha\r\n    $ SPARK_HADOOP_VERSION=2.0.5-alpha SPARK_YARN=true sbt/sbt assembly\r\n\r\n    # Cloudera CDH 4.2.0 with MapReduce v2\r\n    $ SPARK_HADOOP_VERSION=2.0.0-cdh4.2.0 SPARK_YARN=true sbt/sbt assembly\r\n\r\nFor convenience, these variables may also be set through the\r\n`conf/spark-env.sh` file described below.\r\n\r\nWhen developing a Spark application, specify the Hadoop version by\r\nadding the \"hadoop-client\" artifact to your project's\r\ndependencies. For example, if you're using Hadoop 1.0.1 and build your\r\napplication using SBT, add this entry to `libraryDependencies`:\r\n\r\n    \"org.apache.hadoop\" % \"hadoop-client\" % \"1.2.1\"\r\n\r\nIf your project is built with Maven, add this to your POM file's\r\n`<dependencies>` section:\r\n\r\n    <dependency>\r\n      <groupId>org.apache.hadoop</groupId>\r\n      <artifactId>hadoop-client</artifactId>\r\n      <version>1.2.1</version>\r\n    </dependency>\r\n\r\n\r\n## Configuration\r\n\r\nPlease refer to the [Configuration\r\nguide](http://spark.incubator.apache.org/docs/latest/configuration.html)\r\nin the online documentation for an overview on how to configure Spark.\r\n\r\n\r\n## Contributing to GraphX\r\n\r\nContributions via GitHub pull requests are gladly accepted from their\r\noriginal author. Along with any pull requests, please state that the\r\ncontribution is your original work and that you license the work to\r\nthe project under the project's open source license. Whether or not\r\nyou state this explicitly, by submitting any copyrighted material via\r\npull request, email, or other means you agree to license the material\r\nunder the project's open source license and warrant that you have the\r\nlegal authority to do so.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}